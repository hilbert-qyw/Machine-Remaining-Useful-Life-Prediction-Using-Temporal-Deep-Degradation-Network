# Machine Remaining Useful Life Prediction Using Temporal Deep Degradation Network with Attention-based Dynamic Degradation Patterns

Author: Yuwen Qin, Xin Chen

## Background
Prognostics maintenance is challenging due to machines' increasingly nonlinear complexity and uncertainty. Prognostics and health management (PHM) technology is designed to make maintenance decisions to improve the reliability, stability, and efficiency of machines. As a technical term that describes the development of faults in PHM, the remaining useful life (RUL) is defined as the period from the current time to the failure within a component. The accurate RUL prediction plays a vital role in PHM that monitors and detects the health states of machines to provide precise failure warnings and avoid safety accidents<sup>[1-2]</sup>.

We propose a novel data-driven method named temporal deep degradation network (TDDN) for RUL prediction. The TDDN model consists of data transformation, deep separable convolutional network (DSCN), and attention mechanism modules. Specially, the data transformation module and DSCN module are designed to extract abstract features for modeling temporal and spatial correlations in streaming sensor data. The attention mechanism module is adopted to calculate the attention weights of abstract feature components to capture dynamic degradation patterns for RUL prediction. Finally, the TDDN model outperforms the existing deep learning neural network models on four subsets in the commercial modular aero-propulsion system simulation (C-MAPSS) dataset. The main contributions of this paper are as follows:

- The TDDN model is proposed for the RUL prediction of machines. The hyperparameters of TDDN model are optimized by hyperband algorithm automatically.
- Deep separable convolutional networks are adopted to learn temporal and spatial features effectively from degradation-related derived data in time and frequency domains, which extracts richer degradation features from high-dimension sensor data.
- The dynamic patterns of machine degradation are encoded by the attention mechanism into the attention map which can serve as a fingerprint of the degradation under different operating conditions. The degradation fingerprint can significantly improve the accuracy of RUL prediction.
- The comparative experiments and ablation studies are conducted on the benchmark C-MAPSS dataset. The TDDN model achieves better performance on four subsets than other models.

## Data

### Introduction

The C-MAPSS dataset is widely used to evaluate the performance of models. It is generated by a thermo-dynamical simulation model which simulates damage propagation and performance degradation in MATLAB and Simulink environment. The dataset is divided into four subsets according to engine operating conditions and fault modes. Four subsets are denoted as FD001, FD002, FD003, and FD004, respectively. In each subset, engine number, operating cycle number, three operating settings, and 21 sensor measurements are recorded to reflect turbofan engine degradation. The detailed description of the training and test dataset for each subset can be found in the previous work<sup>[3-5]</sup>. The operating conditions, fault modes, and other statistical data of each subset are listed in Table 1. The sensor selection, data normalization, moving windows, and padding strategy are implemented for the data preprocessing to improve the computational efficiency of RUL prediction. 

<center>Table. 1 . Information of the C-MAPSS dataset<center>

|           Dataset           | FD001 | FD002 | FD003 | FD004 |
| :-------------------------: | :---: | :---: | :---: | :---: |
|  Engines in training data   |  100  |  260  |  100  |  259  |
|    Engines in test data     |  100  |  259  |  100  |  248  |
|    Operating conditions     |   1   |   6   |   1   |   6   |
|         Fault modes         |   1   |   1   |   2   |   2   |
| Training minimum life cycle |  128  |  128  |  145  |  128  |
|   Test minimum life cycle   |  31   |  21   |  38   |  19   |

### Feature Engineering

The sensor selection of FD001 and FD003 subsets are different  from that of the FD002 and FD004 subsets. In FD001 and FD003 subsets, there are twenty-one sensors labeled with the indices of $(1,\cdots, 21)$ and three operating settings of s1,s2, and s3.
The twenty-one sensors and operating settings in FD001 and FD004 subset are visualized in Fig. 1. The streaming data in sensors and settings with the ascending and descending trends in FD001 and FD003 ( see Table. 2) are used as inputs for the TDDN model. All operating settings and sensor measurements are used for the RUL prediction of FD002 and FD004 subsets.

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/visualization_of_sensor_data.png?token=GHSAT0AAAAAAB6EOLSDRNHALCZ5YYSNGST4Y66D7UQ" alt="image-20230203114211407" style="zoom: 70%;" />

<center>Fig. 1 . The visualization of the streaming sensor data and settings in FD001 and FD004 subsets<center>
   <center>Table. 2 .  The trend categories of the sensors and operating settings in FD001 and FD003<center> 

|   Trend    |                Sensors                |
| :--------: | :-----------------------------------: |
| Ascending  | 2, 3, 4, 8, 9, 11, 13, 15, 17, s1, s2 |
| Descending |         7, 12, 20, 21, s1, s2         |
| Unchanged  |    1, 5, 6, 10, 14, 16, 18, 19, s3    |

## Temporal Deep Degradation Network

The overall structure of the TDDN model is shown in Fig. 2. First, the data transformation module has three components and is used to generate degradation-related derived data. The identity mapping, down-sampling, and exponential moving averages are arranged in parallel to help extract degradation patterns from the streaming sensor data individually. Next, the DSCN modules are separately utilized for each degradation-related derived data to extract abstract features via multiple separable convolutional blocks (SCB) and adaptive pooling. Finally, the attention mechanism module is employed to assign higher weights to more important abstract feature components. The output of the attention mechanism is fed into feed-forward neural network (FNN) layers to predict RUL. In addition, a hyperband algorithm is introduced to automatically search the optimal hyperparameters of the TDDN model to improve RUL prediction performance<sup>[6]</sup>. 

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/TDDN_model.png?token=GHSAT0AAAAAAB6EOLSC7YGZ5XOFDKW46YL4Y66EAHA" />

<center>Fig. 2 . Architecture of the proposed TDDN for RUL prediction<center>

### Deep Separable Convolutional Network

A standard CNN generally comprises convolutional layers, pooling layers, and a fully connected layer.In convolutional layers, each kernel attaches equal importance to intra-sensor temporal features and inter-sensor spatial features, which ignores the discrepancies of features in different physical sensor data. In addition, the standard convolution always leads to high computational complexity and memory budget with the increasing number of network layers. To deal with those problems, we adopt the DSCN to enhance the feature capability of the TDDN model, in which the SCB is introduced to replace the standard convolutional layer<sup>[7]</sup>. As shown in Fig. 3, the DSCN contains three main structures: SCB, residual connection, and adaptive pooling.

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/DSCN.png?token=GHSAT0AAAAAAB6EOLSC2BBG4766XR2MJEYAY66D7BQ" alt="image-20230203115403367" style="zoom:67%;" />

<center>Fig. 3 . The illustration of DSCN for abstract feature extraction<center>

### Attention Mechanism 

The attention mechanism is adopted to calculate the degradation attention weights to encode the dynamic degradation patterns<sup>[8]</sup>. The attention mechanism generates degradation attention weights according to the similarities between abstract feature components and the degradation stages. The overall structure of the attention mechanism is shown in Fig. 4.

<img src="[C:\Users\11855\AppData\Roaming\Typora\typora-user-images\image-20230203115608549.png](https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/AM.png?token=GHSAT0AAAAAAB6EOLSC2V5YEDRZQKDFOBOEY66EA2A)" alt="image-20230203115608549" style="zoom:67%;" />

<center>Fig. 4 . The illustration of the overall structure of attention mechanism<center>
## Requirements

```python
numpy~=1.19.2
pandas~=1.4.0
matplotlib~=3.3.2
torch~=1.9.0
scikit-learn~=0.23.2
tqdm~=4.50.2
scipy~=1.5.2
xlrd~=1.2.0
openpyxl~=3.0.5
```

Dependencies can be installed using the following command:

```python
pip install -r requirements.txt
```

## Usage

```python
python TDRL_model_training.py --file_number 1 --max_life 1 --time 1 --norm symmetric
# The detailed descriptions about the arguments are as following:
# file_number: the order of file
# max_life: normalization
# times: number of training
# norm: Type of parameter normalization ([0, 1] or [-1, 1])
```

## Results

The predicted RUL results of test engines in four subsets are shown in Fig. 5.

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/RUL_prediction.png?token=GHSAT0AAAAAAB6EOLSDQAC57P4D7IVPW3JYY66EBFA" alt="image-20230203115957494" style="zoom:67%;" />

<center>Fig. 5 . RUL prediction results of four subsets with the TDDN model<center>

One representative engine is randomly selected to visualize the prediction performance in four subsets of FD001, FD002, FD003, and FD004. Fig. 6 shows the RUL prediction results and the corresponding absolute errors of four subsets with TDDN, DCNN, LSTM and CNN-LSTM models. It is observed that the prediction results of four models evolve around the actual RUL curve, among which the TDDN model has the smallest deviation range. Furthermore, there are some abnormal fluctuations in the degradation stage in the other three models' prediction results, while TDDN model prediction results always fluctuate within small errors. These results suggest that the TDDN model is effective to capture the trend of the degradation development and has outstanding prediction performance under various operating conditions.

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/FD001.png?token=GHSAT0AAAAAAB6EOLSCD37EV3563A4TOTYKY66EBQA" alt="image-20230203120134378" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/FD002.png?token=GHSAT0AAAAAAB6EOLSCAKDQRLR4VWVBGZL6Y66EBRQ" alt="image-20230203120152014" style="zoom:67%;" />



<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/FD003.png?token=GHSAT0AAAAAAB6EOLSDURYD7U5DAWF6UHY6Y66EBSA" alt="image-20230203120224089" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/hilbert-qyw/Machine-Remaining-Useful-Life-Prediction-Using-Temporal-Deep-Degradation-Network/main/images/FD004.png?token=GHSAT0AAAAAAB6EOLSC4YJ4YYQFX7TWAZOQY66EBTA" alt="image-20230203120246658" style="zoom:67%;" />

<center>Fig. 6 .  The predicted RUL and prediction errors on different test datasets<center>


## Citation

```
@article{qin2022remaining,
  title={Remaining Useful Life Prediction Using Temporal Deep Degradation Network for Complex Machinery with Attention-based Feature Extraction},
  author={Qin, Yuwen and Cai, Ningbo and Gao, Chen and Zhang, Yadong and Cheng, Yonghong and Chen, Xin},
  journal={arXiv preprint arXiv:2202.10916},
  year={2022}
}
```



## Reference

<div id="refer-anchor-1"></div>

- [1] Y. Hu, X. Miao, Y. Si, E. Pan, E. Zio, Prognostics and health management: A review from the perspectives of design, development and decision, Reliability Engineering & System Safety 217 (2022) 108063.

<div id="refer-anchor-2"></div>

- [2] R. Gong, J. Li, C. Wang, Remaining useful life prediction based on multi-sensor fusion and attention tcn-bigru model, IEEE Sensors Journal (2022).

<div id="refer-anchor-3"></div>

- [3] A. Saxena, K. Goebel, D. Simon, N. Eklund, Damage propagation modeling for aircraft engine run-to-failure simulation, in: 2008 international conference on prognostics and health management, IEEE, 2008, pp. 1–9. (https://data.nasa.gov/dataset/Damage-Propagation-Modeling-for-Aircraft-Engine-Ru/j94x-wgir)

<div id="refer-anchor-4"></div>

- [4] F. O. Heimes, Recurrent neural networks for remaining useful life estimation, in: 2008 international conference on prognostics and health management, IEEE, 2008, pp. 1–6.

  <div id="refer-anchor-5"></div>

- [5] Data Repository (https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan), NASA Ames Research Center, Moffett Field, CA

<div id="refer-anchor-6"></div>

- [6] H. Mo, L. L. Custode, G. Iacca, Evolutionary neural architecture search for remaining useful life prediction, Applied Soft Computing 108 (2021) 107474.

<div id="refer-anchor-7"></div>

- [7] F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1251–1258.

<div id="refer-anchor-8"></div>

- [8] W. Wang, Z. Chen, H. Hu, Hierarchical attention network for image captioning, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, pp. 8957–8964.

<div id="refer-anchor-9"></div>
